"""
RSSè·å–å’Œç®¡ç†æ¨¡å—
"""
import hashlib
import json
import os
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Set

import feedparser
import requests

from ..core.config import Config
from ..core.utils import setup_logger
from .image_service import ImageDownloader

logger = setup_logger(__name__)


class RSSItem:
    """RSSæ¡ç›®æ•°æ®ç±»"""

    def __init__(self, title: str, link: str, description: str, published: datetime):
        self.title = title
        self.link = link
        self.description = description
        self.published = published
        self.title_hash = self._generate_title_hash(title)
        self.date_key = published.strftime("%Y-%m-%d")
        self.sent_status = False  # æ˜¯å¦å·²å‘é€
        self.sent_time: Optional[datetime] = None  # å‘é€æ—¶é—´
        self.quality_score: Optional[int] = None  # AIè´¨é‡è¯„åˆ†ï¼ˆ0-10åˆ†ï¼‰
        self.scored_time: Optional[datetime] = None  # è¯„åˆ†æ—¶é—´
        
        # è´¨é‡æ§åˆ¶çŠ¶æ€ - ç®€åŒ–è®¾è®¡ï¼Œä¸»è¦åŸºäºquality_score
        self.excluded_from_sending: bool = False  # æ˜¯å¦è¢«æ’é™¤å‡ºå‘é€é˜Ÿåˆ—
        self.exclusion_reason: Optional[str] = None  # æ’é™¤åŸå› 
        
        # å‘é€çŠ¶æ€è¯¦ç»†è®°å½•
        self.send_attempts: int = 0  # å‘é€å°è¯•æ¬¡æ•°
        self.last_attempt_time: Optional[datetime] = None  # æœ€åå°è¯•æ—¶é—´
        self.send_error: Optional[str] = None  # å‘é€é”™è¯¯ä¿¡æ¯
        self.send_success: bool = False  # å‘é€æ˜¯å¦æˆåŠŸ
        
        # å›¾ç‰‡ç›¸å…³å±æ€§
        self.image_url: Optional[str] = None  # åŸå§‹å›¾ç‰‡URL
        self.local_image_path: Optional[str] = None  # æœ¬åœ°å›¾ç‰‡è·¯å¾„
        self.image_downloaded: bool = False  # å›¾ç‰‡æ˜¯å¦å·²ä¸‹è½½
        
        # RSSæºä¿¡æ¯
        self.source_name: Optional[str] = None  # RSSæºåç§°
        self.source_url: Optional[str] = None   # RSSæºURL

    def _generate_title_hash(self, title: str) -> str:
        """ç”Ÿæˆæ ‡é¢˜çš„å”¯ä¸€æ ‡è¯†ç¬¦"""
        # æ¸…ç†æ ‡é¢˜ï¼Œå»é™¤å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
        cleaned_title = " ".join(title.strip().split())
        return hashlib.md5(cleaned_title.encode("utf-8")).hexdigest()[:16]

    def mark_as_sent(self) -> None:
        """æ ‡è®°ä¸ºå·²å‘é€æˆåŠŸ"""
        self.sent_status = True
        self.sent_time = datetime.now()
        self.send_success = True
        self.send_error = None

    def mark_send_failed(self, error_message: str) -> None:
        """æ ‡è®°å‘é€å¤±è´¥"""
        self.send_attempts += 1
        self.last_attempt_time = datetime.now()
        self.send_error = error_message
        self.send_success = False
        # å¦‚æœå°è¯•æ¬¡æ•°è¿‡å¤šï¼Œæ ‡è®°ä¸ºå·²å¤„ç†é¿å…é‡å¤å°è¯•
        if self.send_attempts >= 3:
            self.sent_status = True  # æ ‡è®°ä¸ºå·²å¤„ç†ï¼Œä½†ä¸æ˜¯æˆåŠŸå‘é€

    def mark_send_attempt(self) -> None:
        """è®°å½•å‘é€å°è¯•"""
        self.send_attempts += 1
        self.last_attempt_time = datetime.now()

    def should_retry_send(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥é‡è¯•å‘é€"""
        if self.sent_status:  # å·²ç»å¤„ç†è¿‡ï¼ˆæˆåŠŸæˆ–å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼‰
            return False
        if self.send_attempts >= 3:  # æœ€å¤šå°è¯•3æ¬¡
            return False
        # å¦‚æœä¸Šæ¬¡å°è¯•æ—¶é—´è·ç¦»ç°åœ¨ä¸åˆ°5åˆ†é’Ÿï¼Œä¸é‡è¯•
        if self.last_attempt_time and (datetime.now() - self.last_attempt_time).total_seconds() < 300:
            return False
        return True

    def set_quality_score(self, score: int) -> None:
        """è®¾ç½®è´¨é‡è¯„åˆ†"""
        self.quality_score = max(0, min(10, score))  # ç¡®ä¿åˆ†æ•°åœ¨0-10èŒƒå›´å†…
        self.scored_time = datetime.now()
        
        logger.debug(f"ğŸ¯ æ–‡ç« è¯„åˆ†è®¾ç½®: {self.title[:30]}... -> {self.quality_score}/10")
        
        # æ£€æŸ¥æ˜¯å¦é€šè¿‡è´¨é‡è¦æ±‚ï¼Œå¦‚æœä¸é€šè¿‡åˆ™è‡ªåŠ¨æ’é™¤
        from ..core.config import Config
        min_score = getattr(Config, 'MIN_QUALITY_SCORE', 7)
        if self.quality_score < min_score:
            reason = f"è´¨é‡è¯„åˆ† {self.quality_score} ä½äºè¦æ±‚ {min_score}"
            logger.warning(f"ğŸš« æ–‡ç« è¢«è‡ªåŠ¨æ’é™¤: {self.title[:30]}... åŸå› : {reason}")
            self.exclude_from_sending(reason)
        else:
            logger.debug(f"âœ… æ–‡ç« é€šè¿‡è´¨é‡æ£€æŸ¥: {self.title[:30]}... åˆ†æ•°: {self.quality_score}/{min_score}")

    def exclude_from_sending(self, reason: str) -> None:
        """å°†æ–‡ç« æ’é™¤å‡ºå‘é€é˜Ÿåˆ—"""
        self.excluded_from_sending = True
        self.exclusion_reason = reason
        logger.info(f"âŒ æ–‡ç« å·²æ’é™¤å‡ºå‘é€é˜Ÿåˆ—: {self.title[:50]}... åŸå› : {reason}")

    def is_sendable(self) -> bool:
        """æ£€æŸ¥æ–‡ç« æ˜¯å¦å¯ä»¥å‘é€"""
        # å·²å‘é€æˆ–å‘é€æˆåŠŸçš„ä¸å†å‘é€
        if self.sent_status or self.send_success:
            logger.debug(f"æ–‡ç« å·²å‘é€: {self.title[:30]}... sent_status={self.sent_status}, send_success={self.send_success}")
            return False
            
        # è¢«æ‰‹åŠ¨æ’é™¤å‡ºå‘é€é˜Ÿåˆ—çš„ä¸å‘é€
        if self.excluded_from_sending:
            logger.debug(f"æ–‡ç« è¢«æ’é™¤: {self.title[:30]}... åŸå› : {self.exclusion_reason}")
            return False
            
        # å¦‚æœå·²è¯„åˆ†ä¸”åˆ†æ•°ä¸è¾¾æ ‡ï¼Œä¸å‘é€
        if self.has_quality_score() and not self.meets_quality_requirement():
            logger.info(f"âŒ æ–‡ç« è´¨é‡ä¸è¾¾æ ‡å·²æ’é™¤: {self.title[:30]}... åˆ†æ•°: {self.quality_score} (éœ€è¦â‰¥{Config.MIN_QUALITY_SCORE})")
            return False
            
        # æ£€æŸ¥é‡è¯•é€»è¾‘
        if not self.should_retry_send():
            logger.debug(f"æ–‡ç« é‡è¯•é™åˆ¶: {self.title[:30]}... å°è¯•æ¬¡æ•°: {self.send_attempts}, ä¸Šæ¬¡å°è¯•: {self.last_attempt_time}")
            return False
            
        logger.debug(f"æ–‡ç« å¯å‘é€: {self.title[:30]}... è´¨é‡åˆ†: {self.quality_score}")
        return True

    def has_quality_score(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²æœ‰è´¨é‡è¯„åˆ†"""
        return self.quality_score is not None and self.scored_time is not None

    def meets_quality_requirement(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ»¡è¶³è´¨é‡è¦æ±‚"""
        if not self.has_quality_score():
            return True  # æœªè¯„åˆ†çš„æ–‡ç« é»˜è®¤è®¤ä¸ºå¯å‘é€ï¼Œç­‰å¾…è¯„åˆ†
            
        from ..core.config import Config
        min_score = getattr(Config, 'MIN_QUALITY_SCORE', 7)
        return self.quality_score >= min_score

    def needs_quality_check(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦è¿›è¡Œè´¨é‡æ£€æŸ¥"""
        from ..core.config import Config
        
        # å¦‚æœç¦ç”¨äº†è´¨é‡æ£€æŸ¥ï¼Œä¸éœ€è¦æ£€æŸ¥
        if not getattr(Config, 'ENABLE_QUALITY_CHECK', True):
            return False
            
        # å·²ç»æœ‰è¯„åˆ†çš„ä¸éœ€è¦å†æ£€æŸ¥
        if self.has_quality_score():
            return False
            
        return True

    def set_image_info(self, image_url: str, local_path: str = None) -> None:
        """è®¾ç½®å›¾ç‰‡ä¿¡æ¯"""
        self.image_url = image_url
        if local_path:
            self.local_image_path = local_path
            self.image_downloaded = True

    def has_image(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰å›¾ç‰‡"""
        return self.image_url is not None

    def has_local_image(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰æœ¬åœ°å›¾ç‰‡"""
        return self.local_image_path is not None and self.image_downloaded

    def __str__(self):
        return f"{self.title} - {self.link}"

    def to_dict(self) -> dict:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ç”¨äºåºåˆ—åŒ–"""
        return {
            "title": self.title,
            "link": self.link,
            "description": self.description,
            "published": self.published.isoformat(),
            "title_hash": self.title_hash,
            "date_key": self.date_key,
            "sent_status": self.sent_status,
            "sent_time": self.sent_time.isoformat() if self.sent_time else None,
            "quality_score": self.quality_score,
            "scored_time": self.scored_time.isoformat() if self.scored_time else None,
            # è´¨é‡æ§åˆ¶çŠ¶æ€å­—æ®µ - ç®€åŒ–
            "excluded_from_sending": self.excluded_from_sending,
            "exclusion_reason": self.exclusion_reason,
            # å‘é€çŠ¶æ€å­—æ®µ
            "send_attempts": self.send_attempts,
            "last_attempt_time": self.last_attempt_time.isoformat() if self.last_attempt_time else None,
            "send_error": self.send_error,
            "send_success": self.send_success,
            # å›¾ç‰‡å’Œæºä¿¡æ¯
            "image_url": self.image_url,
            "local_image_path": self.local_image_path,
            "image_downloaded": self.image_downloaded,
            "source_name": self.source_name,
            "source_url": self.source_url,
        }

    @classmethod
    def from_dict(cls, data: dict) -> "RSSItem":
        """ä»å­—å…¸åˆ›å»ºRSSæ¡ç›®"""
        item = cls(
            title=data["title"],
            link=data["link"],
            description=data["description"],
            published=datetime.fromisoformat(data["published"]),
        )
        item.sent_status = data.get("sent_status", False)
        if data.get("sent_time"):
            item.sent_time = datetime.fromisoformat(data["sent_time"])
        item.quality_score = data.get("quality_score")
        if data.get("scored_time"):
            item.scored_time = datetime.fromisoformat(data["scored_time"])
        
        # æ¢å¤è´¨é‡æ§åˆ¶çŠ¶æ€ - ç®€åŒ–
        item.excluded_from_sending = data.get("excluded_from_sending", False)
        item.exclusion_reason = data.get("exclusion_reason")
        
        # æ¢å¤å‘é€çŠ¶æ€ä¿¡æ¯
        item.send_attempts = data.get("send_attempts", 0)
        if data.get("last_attempt_time"):
            item.last_attempt_time = datetime.fromisoformat(data["last_attempt_time"])
        item.send_error = data.get("send_error")
        item.send_success = data.get("send_success", False)
        
        # æ¢å¤å›¾ç‰‡ä¿¡æ¯
        item.image_url = data.get("image_url")
        item.local_image_path = data.get("local_image_path")
        item.image_downloaded = data.get("image_downloaded", False)
        
        # æ¢å¤æºä¿¡æ¯
        item.source_name = data.get("source_name")
        item.source_url = data.get("source_url")
        
        return item


class RSSCache:
    """RSSç¼“å­˜ç®¡ç†å™¨"""

    def __init__(self, cache_dir: str = "cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.daily_cache: Dict[str, Set[str]] = {}  # date -> set of title_hashes
        self.article_details: Dict[
            str, Dict[str, RSSItem]
        ] = {}  # date -> hash -> RSSItem
        self._load_cache()

    def _get_cache_file(self, date_key: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        return self.cache_dir / f"rss_{date_key}.json"

    def _load_cache(self):
        """åŠ è½½ç¼“å­˜æ•°æ®"""
        # åŠ è½½ä»Šå¤©å’Œæ˜¨å¤©çš„ç¼“å­˜
        for days_back in [0, 1]:
            date = (datetime.now() - timedelta(days=days_back)).strftime("%Y-%m-%d")
            cache_file = self._get_cache_file(date)

            if cache_file.exists():
                try:
                    with open(cache_file, "r", encoding="utf-8") as f:
                        data = json.load(f)

                        # åŠ è½½åŸºæœ¬ç¼“å­˜ä¿¡æ¯
                        self.daily_cache[date] = set(data.get("title_hashes", []))

                        # åŠ è½½æ–‡ç« è¯¦ç»†ä¿¡æ¯
                        self.article_details[date] = {}
                        for item_data in data.get("articles", []):
                            item = RSSItem.from_dict(item_data)
                            self.article_details[date][item.title_hash] = item

                        logger.debug(f"åŠ è½½ç¼“å­˜ {date}: {len(self.daily_cache[date])} æ¡è®°å½•")
                except Exception as e:
                    logger.error(f"åŠ è½½ç¼“å­˜æ–‡ä»¶å¤±è´¥ {cache_file}: {e}")
                    self.daily_cache[date] = set()
                    self.article_details[date] = {}
            else:
                self.daily_cache[date] = set()
                self.article_details[date] = {}

    def _save_cache(self, date_key: str):
        """ä¿å­˜ç¼“å­˜æ•°æ®"""
        if date_key not in self.daily_cache:
            return

        cache_file = self._get_cache_file(date_key)
        try:
            # æ”¶é›†æ–‡ç« è¯¦ç»†ä¿¡æ¯
            articles = []
            if date_key in self.article_details:
                for item in self.article_details[date_key].values():
                    articles.append(item.to_dict())

            data = {
                "date": date_key,
                "title_hashes": list(self.daily_cache[date_key]),
                "articles": articles,
                "updated_at": datetime.now().isoformat(),
            }

            with open(cache_file, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)

            logger.debug(f"ä¿å­˜ç¼“å­˜ {date_key}: {len(self.daily_cache[date_key])} æ¡è®°å½•")
        except Exception as e:
            logger.error(f"ä¿å­˜ç¼“å­˜æ–‡ä»¶å¤±è´¥ {cache_file}: {e}")

    def is_duplicate(self, item: RSSItem) -> bool:
        """æ£€æŸ¥æ–‡ç« æ˜¯å¦é‡å¤"""
        return item.title_hash in self.daily_cache.get(item.date_key, set())

    def add_item(self, item: RSSItem):
        """æ·»åŠ æ–‡ç« åˆ°ç¼“å­˜"""
        if item.date_key not in self.daily_cache:
            self.daily_cache[item.date_key] = set()
            self.article_details[item.date_key] = {}

        self.daily_cache[item.date_key].add(item.title_hash)
        self.article_details[item.date_key][item.title_hash] = item
        self._save_cache(item.date_key)

    def update_item_sent_status(self, item: RSSItem):
        """æ›´æ–°æ–‡ç« å‘é€çŠ¶æ€"""
        if (
            item.date_key in self.article_details
            and item.title_hash in self.article_details[item.date_key]
        ):
            self.article_details[item.date_key][
                item.title_hash
            ].sent_status = item.sent_status
            self.article_details[item.date_key][
                item.title_hash
            ].sent_time = item.sent_time
            self._save_cache(item.date_key)

    def get_unsent_items(self, date_key: str = None) -> List[RSSItem]:
        """è·å–æœªå‘é€ä¸”å¯å‘é€çš„æ–‡ç« """
        sendable_items = []
        total_items = 0
        debug_stats = {
            'sent_status': 0,
            'excluded': 0,
            'quality_failed': 0,
            'retry_failed': 0,
            'sendable': 0
        }

        if date_key:
            # è·å–æŒ‡å®šæ—¥æœŸçš„å¯å‘é€æ–‡ç« 
            if date_key in self.article_details:
                for item in self.article_details[date_key].values():
                    total_items += 1
                    if item.sent_status or item.send_success:
                        debug_stats['sent_status'] += 1
                    elif item.excluded_from_sending:
                        debug_stats['excluded'] += 1
                        logger.debug(f"æ–‡ç« è¢«æ’é™¤: {item.title[:30]}... åŸå› : {item.exclusion_reason}")
                    elif item.has_quality_score() and not item.meets_quality_requirement():
                        debug_stats['quality_failed'] += 1
                        logger.info(f"âŒ æ–‡ç« è´¨é‡ä¸è¾¾æ ‡å·²æ’é™¤: {item.title[:30]}... åˆ†æ•°: {item.quality_score} (éœ€è¦â‰¥{Config.MIN_QUALITY_SCORE})")
                    elif not item.should_retry_send():
                        debug_stats['retry_failed'] += 1
                        logger.debug(f"æ–‡ç« é‡è¯•æ¬¡æ•°è¿‡å¤š: {item.title[:30]}... å°è¯•: {item.send_attempts}")
                    else:
                        debug_stats['sendable'] += 1
                        sendable_items.append(item)
        else:
            # è·å–æ‰€æœ‰æ—¥æœŸçš„å¯å‘é€æ–‡ç« 
            for date_articles in self.article_details.values():
                for item in date_articles.values():
                    total_items += 1
                    if item.sent_status or item.send_success:
                        debug_stats['sent_status'] += 1
                    elif item.excluded_from_sending:
                        debug_stats['excluded'] += 1
                        logger.debug(f"æ–‡ç« è¢«æ’é™¤: {item.title[:30]}... åŸå› : {item.exclusion_reason}")
                    elif item.has_quality_score() and not item.meets_quality_requirement():
                        debug_stats['quality_failed'] += 1
                        logger.info(f"âŒ æ–‡ç« è´¨é‡ä¸è¾¾æ ‡å·²æ’é™¤: {item.title[:30]}... åˆ†æ•°: {item.quality_score} (éœ€è¦â‰¥{Config.MIN_QUALITY_SCORE})")
                    elif not item.should_retry_send():
                        debug_stats['retry_failed'] += 1
                        logger.debug(f"æ–‡ç« é‡è¯•æ¬¡æ•°è¿‡å¤š: {item.title[:30]}... å°è¯•: {item.send_attempts}")
                    else:
                        debug_stats['sendable'] += 1
                        sendable_items.append(item)

        logger.info(f"ğŸ“Š æ–‡ç« çŠ¶æ€ç»Ÿè®¡ - æ€»è®¡: {total_items}, å¯å‘é€: {debug_stats['sendable']}, "
                   f"å·²å‘é€: {debug_stats['sent_status']}, è¢«æ’é™¤: {debug_stats['excluded']}, "
                   f"è´¨é‡ä¸è¾¾æ ‡(å·²æ’é™¤): {debug_stats['quality_failed']}, é‡è¯•å¤±è´¥: {debug_stats['retry_failed']}")

        # è¡¥å……è¯´æ˜ï¼šè´¨é‡ä¸è¾¾æ ‡çš„æ–‡ç« ä¼šè¢«è‡ªåŠ¨æ’é™¤å‡ºå‘é€é˜Ÿåˆ—
        if debug_stats['quality_failed'] > 0:
            logger.info(f"â„¹ï¸ è¯´æ˜ï¼š{debug_stats['quality_failed']}ç¯‡è´¨é‡ä¸è¾¾æ ‡æ–‡ç« å·²è¢«è‡ªåŠ¨æ’é™¤ï¼Œä¸ä¼šè¿›å…¥å‘é€é˜Ÿåˆ—")

        # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        sendable_items.sort(key=lambda x: x.published, reverse=True)
        return sendable_items

    def get_items_needing_quality_check(self) -> List[RSSItem]:
        """è·å–éœ€è¦è´¨é‡æ£€æŸ¥çš„æ–‡ç« """
        items_to_check = []
        
        for date_articles in self.article_details.values():
            for item in date_articles.values():
                if item.needs_quality_check():
                    items_to_check.append(item)
        
        # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        items_to_check.sort(key=lambda x: x.published, reverse=True)
        return items_to_check

    def get_excluded_items(self) -> List[RSSItem]:
        """è·å–è¢«æ’é™¤å‡ºå‘é€é˜Ÿåˆ—çš„æ–‡ç« """
        excluded_items = []
        
        for date_articles in self.article_details.values():
            for item in date_articles.values():
                if item.excluded_from_sending:
                    excluded_items.append(item)
        
        # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        excluded_items.sort(key=lambda x: x.published, reverse=True)
        return excluded_items

    def cleanup_old_cache(self, keep_days: int = 7):
        """æ¸…ç†æ—§çš„ç¼“å­˜æ–‡ä»¶"""
        cutoff_date = datetime.now() - timedelta(days=keep_days)

        for cache_file in self.cache_dir.glob("rss_*.json"):
            try:
                # ä»æ–‡ä»¶åæå–æ—¥æœŸ
                date_str = cache_file.stem.replace("rss_", "")
                file_date = datetime.strptime(date_str, "%Y-%m-%d")

                if file_date < cutoff_date:
                    cache_file.unlink()
                    logger.info(f"åˆ é™¤æ—§ç¼“å­˜æ–‡ä»¶: {cache_file}")

                    # ä»å†…å­˜ä¸­ç§»é™¤
                    if date_str in self.daily_cache:
                        del self.daily_cache[date_str]

            except Exception as e:
                logger.error(f"æ¸…ç†ç¼“å­˜æ–‡ä»¶å¤±è´¥ {cache_file}: {e}")


class RSSFetcher:
    """RSSè·å–å™¨"""

    def __init__(self, feed_url: str):
        if not feed_url:
            raise ValueError("RSS feed URL is required")
        self.feed_url = feed_url
        self.last_check_time: Optional[datetime] = None
        self.cache = RSSCache()
        self.image_downloader = ImageDownloader()  # åˆå§‹åŒ–å›¾ç‰‡ä¸‹è½½å™¨

    def fetch_latest_items(
        self, since_minutes: int = None, enable_dedup: bool = True
    ) -> List[RSSItem]:
        """
        è·å–æœ€æ–°çš„RSSæ¡ç›®

        Args:
            since_minutes: è·å–å¤šå°‘åˆ†é’Ÿå†…çš„æ–‡ç« ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®çš„æ–‡ç« è·å–æ—¶é—´èŒƒå›´
            enable_dedup: æ˜¯å¦å¯ç”¨å»é‡åŠŸèƒ½

        Returns:
            RSSæ¡ç›®åˆ—è¡¨
        """
        try:
            logger.info(f"å¼€å§‹è·å–RSSæ•°æ®: {self.feed_url}")

            # è·å–ä»£ç†é…ç½®
            proxies = Config.get_proxies()
            if proxies:
                logger.info(f"ä½¿ç”¨ä»£ç†: {proxies}")

            # è·å–RSSæ•°æ®
            response = requests.get(
                self.feed_url, 
                timeout=30,
                proxies=proxies,
                headers={
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                }
            )
            response.raise_for_status()

            # è§£æRSS
            feed = feedparser.parse(response.content)

            if feed.bozo:
                logger.warning(f"RSSè§£æè­¦å‘Š: {feed.bozo_exception}")

            cutoff_time = datetime.now() - timedelta(minutes=Config.FETCH_ARTICLES_HOURS * 60)

            logger.info(f"è·å–æœ€è¿‘ {Config.FETCH_ARTICLES_HOURS} å°æ—¶å†…çš„æ–‡ç« ")

            items = []
            duplicate_count = 0

            for entry in feed.entries:
                try:
                    # è§£æå‘å¸ƒæ—¶é—´
                    if hasattr(entry, "published_parsed") and entry.published_parsed:
                        published = datetime(*entry.published_parsed[:6])
                    elif hasattr(entry, "updated_parsed") and entry.updated_parsed:
                        published = datetime(*entry.updated_parsed[:6])
                    else:
                        published = datetime.now()

                    # åªè·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ–‡ç« 
                    if published >= cutoff_time:
                        item = RSSItem(
                            title=entry.get("title", "æ— æ ‡é¢˜"),
                            link=entry.get("link", ""),
                            description=entry.get("description", ""),
                            published=published,
                        )

                        # å°è¯•è·å–å’Œä¸‹è½½å›¾ç‰‡
                        self._process_item_image(item, entry)

                        # æ£€æŸ¥æ˜¯å¦é‡å¤
                        if enable_dedup and self.cache.is_duplicate(item):
                            duplicate_count += 1
                            logger.debug(f"è·³è¿‡é‡å¤æ–‡ç« : {item.title}")
                            continue

                        items.append(item)

                        # æ·»åŠ åˆ°ç¼“å­˜
                        if enable_dedup:
                            self.cache.add_item(item)

                except Exception as e:
                    logger.error(f"è§£æRSSæ¡ç›®æ—¶å‡ºé”™: {e}")
                    continue

            # æ¸…ç†æ—§ç¼“å­˜
            if enable_dedup:
                self.cache.cleanup_old_cache()

            logger.info(f"æˆåŠŸè·å– {len(items)} æ¡æœ€æ–°æ–‡ç« ")
            if duplicate_count > 0:
                logger.info(f"è·³è¿‡ {duplicate_count} æ¡é‡å¤æ–‡ç« ")

            return items

        except requests.RequestException as e:
            logger.error(f"è·å–RSSæ•°æ®å¤±è´¥: {e}")
            return []
        except Exception as e:
            logger.error(f"RSSè§£æå¤±è´¥: {e}")
            return []

    def get_feed_info(self) -> Dict[str, str]:
        """è·å–RSSæºä¿¡æ¯"""
        try:
            response = requests.get(self.feed_url, timeout=30)
            response.raise_for_status()
            feed = feedparser.parse(response.content)

            return {
                "title": feed.feed.get("title", "æœªçŸ¥æº"),
                "description": feed.feed.get("description", ""),
                "link": feed.feed.get("link", ""),
                "last_updated": feed.feed.get("updated", ""),
            }
        except Exception as e:
            logger.error(f"è·å–RSSæºä¿¡æ¯å¤±è´¥: {e}")
            return {}

    def get_cache_status(self) -> Dict[str, any]:
        """è·å–ç¼“å­˜çŠ¶æ€"""
        status = {"cache_dir": str(self.cache.cache_dir), "daily_stats": {}}

        for date_key, title_hashes in self.cache.daily_cache.items():
            status["daily_stats"][date_key] = len(title_hashes)

        # ç»Ÿè®¡ç¼“å­˜æ–‡ä»¶
        cache_files = list(self.cache.cache_dir.glob("rss_*.json"))
        status["cache_files_count"] = len(cache_files)
        status["total_cached_items"] = sum(status["daily_stats"].values())

        return status

    def clear_cache(self, date_key: str = None):
        """æ¸…ç†ç¼“å­˜"""
        if date_key:
            # æ¸…ç†æŒ‡å®šæ—¥æœŸçš„ç¼“å­˜
            if date_key in self.cache.daily_cache:
                del self.cache.daily_cache[date_key]

            cache_file = self.cache._get_cache_file(date_key)
            if cache_file.exists():
                cache_file.unlink()
                logger.info(f"æ¸…ç†ç¼“å­˜: {date_key}")
        else:
            # æ¸…ç†æ‰€æœ‰ç¼“å­˜
            self.cache.daily_cache.clear()
            for cache_file in self.cache.cache_dir.glob("rss_*.json"):
                cache_file.unlink()
            logger.info("æ¸…ç†æ‰€æœ‰ç¼“å­˜")
    
    def _process_item_image(self, item: RSSItem, entry) -> None:
        """
        å¤„ç†æ–‡ç« å›¾ç‰‡
        
        Args:
            item: RSSæ¡ç›®
            entry: feedparseræ¡ç›®å¯¹è±¡
        """
        try:
            # æå–å›¾ç‰‡URL
            image_url = self.image_downloader.extract_image_from_rss_entry(entry)
            
            if image_url:
                logger.info(f"å‘ç°æ–‡ç« å›¾ç‰‡: {item.title[:30]}... -> {image_url}")
                
                # è®¾ç½®å›¾ç‰‡URL
                item.set_image_info(image_url)
                
                # ä¸‹è½½å›¾ç‰‡
                local_path = self.image_downloader.download_image(
                    image_url, 
                    filename=f"{item.title_hash}_{os.path.basename(image_url.split('?')[0])}"
                )
                
                if local_path:
                    # æ›´æ–°æœ¬åœ°è·¯å¾„
                    item.set_image_info(image_url, local_path)
                    logger.info(f"æ–‡ç« å›¾ç‰‡ä¸‹è½½æˆåŠŸ: {local_path}")
                else:
                    logger.warning(f"æ–‡ç« å›¾ç‰‡ä¸‹è½½å¤±è´¥: {image_url}")
            else:
                logger.debug(f"æ–‡ç« æ— å›¾ç‰‡: {item.title[:30]}...")
                
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ç« å›¾ç‰‡å¤±è´¥ {item.title[:30]}...: {e}")
    
    def cleanup_old_images(self, days: int = 30) -> int:
        """
        æ¸…ç†æ—§å›¾ç‰‡
        
        Args:
            days: ä¿ç•™å¤©æ•°
            
        Returns:
            åˆ é™¤çš„æ–‡ä»¶æ•°é‡
        """
        return self.image_downloader.cleanup_old_images(days)
