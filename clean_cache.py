#!/usr/bin/env python3
"""
ç¼“å­˜æ¸…ç†å·¥å…· - æ¸…é™¤æµ‹è¯•æ–‡ç« å’Œè¿‡æœŸæ•°æ®
"""
import sys
import json
from pathlib import Path
from datetime import datetime, timedelta

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from src.services.multi_rss_manager import MultiRSSManager


def clean_test_articles():
    """æ¸…é™¤æµ‹è¯•æ–‡ç« """
    print("=" * 80)
    print("ğŸ§¹ ç¼“å­˜æ¸…ç†å·¥å…· - æ¸…é™¤æµ‹è¯•æ–‡ç« ")
    print("=" * 80)
    
    try:
        # åˆ›å»ºRSSç®¡ç†å™¨
        multi_rss_manager = MultiRSSManager()
        cache = multi_rss_manager.cache
        
        print("ğŸ“Š æ¸…ç†å‰ç¼“å­˜çŠ¶æ€åˆ†æ...")
        
        # ç»Ÿè®¡æ¸…ç†å‰çš„æ•°æ®
        total_articles = 0
        test_articles = 0
        real_articles = 0
        cleaned_articles = []
        
        for date_key in list(cache.article_details.keys()):
            date_articles = cache.article_details[date_key]
            articles_to_remove = []
            
            for article_hash, article in list(date_articles.items()):
                total_articles += 1
                
                # è¯†åˆ«æµ‹è¯•æ–‡ç« çš„æ¡ä»¶
                is_test_article = (
                    article.link.startswith("https://test.com/") or
                    article.title in ["ä¼˜è´¨æ–‡ç« 1", "ä¼˜è´¨æ–‡ç« 2", "ä¸­ç­‰æ–‡ç« 1", "ä½è´¨æ–‡ç« 1", "ä½è´¨æ–‡ç« 2", "ä½è´¨æ–‡ç« 3"] or
                    "æµ‹è¯•æ–‡ç« " in article.description or
                    article.title == "æµ‹è¯•æ ‡é¢˜"
                )
                
                if is_test_article:
                    test_articles += 1
                    articles_to_remove.append(article_hash)
                    cleaned_articles.append({
                        "title": article.title,
                        "link": article.link,
                        "date": date_key
                    })
                    print(f"   ğŸ—‘ï¸ æ ‡è®°æ¸…é™¤: {article.title} ({article.link})")
                else:
                    real_articles += 1
            
            # åˆ é™¤æµ‹è¯•æ–‡ç« 
            for article_hash in articles_to_remove:
                del date_articles[article_hash]
            
            # å¦‚æœè¯¥æ—¥æœŸä¸‹æ²¡æœ‰æ–‡ç« äº†ï¼Œåˆ é™¤æ•´ä¸ªæ—¥æœŸæ¡ç›®å’Œå¯¹åº”çš„ç¼“å­˜æ–‡ä»¶
            if not date_articles:
                del cache.article_details[date_key]
                print(f"   ğŸ“… åˆ é™¤ç©ºæ—¥æœŸæ¡ç›®: {date_key}")
                
                # åˆ é™¤å¯¹åº”çš„ç¼“å­˜æ–‡ä»¶
                cache_file = Path(f"cache/rss_{date_key}.json")
                if cache_file.exists():
                    cache_file.unlink()
                    print(f"   ğŸ—‘ï¸ åˆ é™¤ç©ºç¼“å­˜æ–‡ä»¶: {cache_file.name}")
        
        print(f"\nğŸ“ˆ æ¸…ç†ç»Ÿè®¡:")
        print(f"   æ€»æ–‡ç« æ•°: {total_articles}")
        print(f"   æµ‹è¯•æ–‡ç« : {test_articles}")
        print(f"   çœŸå®æ–‡ç« : {real_articles}")
        print(f"   æ¸…ç†æ–‡ç« : {len(cleaned_articles)}")
        
        # ä¿å­˜æ¸…ç†åçš„ç¼“å­˜
        if cleaned_articles:
            print("\nğŸ’¾ ä¿å­˜æ¸…ç†åçš„ç¼“å­˜...")
            
            # ä¸ºæ¯ä¸ªä¿®æ”¹çš„æ—¥æœŸä¿å­˜ç¼“å­˜
            modified_dates = set()
            for article in cleaned_articles:
                modified_dates.add(article['date'])
            
            for date_key in modified_dates:
                if date_key in cache.article_details:
                    cache._save_cache(date_key)
                    print(f"   âœ… å·²ä¿å­˜ {date_key} çš„ç¼“å­˜")
            
            print("âœ… ç¼“å­˜å·²æ›´æ–°")
            
            # æ˜¾ç¤ºè¢«æ¸…ç†çš„æ–‡ç« 
            print(f"\nğŸ“‹ è¢«æ¸…ç†çš„æ–‡ç« åˆ—è¡¨:")
            for article in cleaned_articles:
                print(f"   - {article['title']} ({article['date']})")
        else:
            print("â„¹ï¸ æ²¡æœ‰å‘ç°éœ€è¦æ¸…ç†çš„æµ‹è¯•æ–‡ç« ")
        
        return len(cleaned_articles)
        
    except Exception as e:
        print(f"âŒ æ¸…ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 0


def clean_old_cache(days_to_keep=7):
    """æ¸…é™¤è¿‡æœŸç¼“å­˜æ–‡ä»¶"""
    print(f"\nğŸ—‚ï¸ æ¸…ç†è¿‡æœŸç¼“å­˜æ–‡ä»¶ (ä¿ç•™ {days_to_keep} å¤©)...")
    
    try:
        cache_dir = Path("cache")
        if not cache_dir.exists():
            print("âš ï¸ ç¼“å­˜ç›®å½•ä¸å­˜åœ¨")
            return 0
        
        cutoff_date = datetime.now() - timedelta(days=days_to_keep)
        deleted_files = []
        
        for cache_file in cache_dir.glob("rss_*.json"):
            try:
                # ä»æ–‡ä»¶åæå–æ—¥æœŸ
                date_str = cache_file.stem.replace("rss_", "")
                file_date = datetime.strptime(date_str, "%Y-%m-%d")
                
                if file_date < cutoff_date:
                    cache_file.unlink()
                    deleted_files.append(cache_file.name)
                    print(f"   ğŸ—‘ï¸ åˆ é™¤è¿‡æœŸæ–‡ä»¶: {cache_file.name}")
                    
            except (ValueError, OSError) as e:
                print(f"   âš ï¸ å¤„ç†æ–‡ä»¶ {cache_file.name} æ—¶å‡ºé”™: {e}")
        
        if deleted_files:
            print(f"âœ… åˆ é™¤äº† {len(deleted_files)} ä¸ªè¿‡æœŸç¼“å­˜æ–‡ä»¶")
        else:
            print("â„¹ï¸ æ²¡æœ‰å‘ç°è¿‡æœŸçš„ç¼“å­˜æ–‡ä»¶")
            
        return len(deleted_files)
        
    except Exception as e:
        print(f"âŒ æ¸…ç†è¿‡æœŸæ–‡ä»¶å¤±è´¥: {e}")
        return 0


def verify_cleanup():
    """éªŒè¯æ¸…ç†ç»“æœ"""
    print(f"\nğŸ” éªŒè¯æ¸…ç†ç»“æœ...")
    
    try:
        multi_rss_manager = MultiRSSManager()
        cache = multi_rss_manager.cache
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æµ‹è¯•æ–‡ç« 
        remaining_test_articles = []
        total_articles = 0
        
        for date_key in cache.article_details:
            for article in cache.article_details[date_key].values():
                total_articles += 1
                
                is_test_article = (
                    article.link.startswith("https://test.com/") or
                    article.title in ["ä¼˜è´¨æ–‡ç« 1", "ä¼˜è´¨æ–‡ç« 2", "ä¸­ç­‰æ–‡ç« 1", "ä½è´¨æ–‡ç« 1", "ä½è´¨æ–‡ç« 2", "ä½è´¨æ–‡ç« 3"] or
                    "æµ‹è¯•æ–‡ç« " in article.description
                )
                
                if is_test_article:
                    remaining_test_articles.append(article.title)
        
        print(f"   æ€»æ–‡ç« æ•°: {total_articles}")
        print(f"   å‰©ä½™æµ‹è¯•æ–‡ç« : {len(remaining_test_articles)}")
        
        if remaining_test_articles:
            print("   âš ï¸ ä»æœ‰æµ‹è¯•æ–‡ç« æœªæ¸…ç†:")
            for title in remaining_test_articles:
                print(f"     - {title}")
            return False
        else:
            print("   âœ… æ‰€æœ‰æµ‹è¯•æ–‡ç« å·²æ¸…ç†å¹²å‡€")
            return True
            
    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {e}")
        return False


if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹ç¼“å­˜æ¸…ç†...")
    
    # æ¸…ç†æµ‹è¯•æ–‡ç« 
    cleaned_count = clean_test_articles()
    
    # æ¸…ç†è¿‡æœŸæ–‡ä»¶
    deleted_count = clean_old_cache(days_to_keep=7)
    
    # éªŒè¯æ¸…ç†ç»“æœ
    is_clean = verify_cleanup()
    
    print("\n" + "=" * 80)
    print("ğŸ“Š æ¸…ç†ç»“æœæ€»ç»“")
    print("=" * 80)
    
    print(f"âœ… æ¸…ç†æµ‹è¯•æ–‡ç« : {cleaned_count} ç¯‡")
    print(f"âœ… åˆ é™¤è¿‡æœŸæ–‡ä»¶: {deleted_count} ä¸ª")
    print(f"âœ… æ¸…ç†çŠ¶æ€: {'å®Œå…¨æ¸…ç†' if is_clean else 'éƒ¨åˆ†æ¸…ç†'}")
    
    if is_clean and (cleaned_count > 0 or deleted_count > 0):
        print("\nğŸ‰ ç¼“å­˜æ¸…ç†å®Œæˆï¼ç³»ç»Ÿç°åœ¨åªåŒ…å«çœŸå®çš„RSSæ–‡ç« ")
    elif cleaned_count == 0 and deleted_count == 0:
        print("\nğŸ’¡ ç¼“å­˜å·²ç»å¾ˆå¹²å‡€ï¼Œæ²¡æœ‰éœ€è¦æ¸…ç†çš„å†…å®¹")
    else:
        print("\nâš ï¸ æ¸…ç†å®Œæˆï¼Œä½†å¯èƒ½è¿˜æœ‰æ®‹ç•™æ•°æ®éœ€è¦æ‰‹åŠ¨æ£€æŸ¥")
